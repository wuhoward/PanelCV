---
title: "Generative vs. Discriminative Models"
date: 2020-05-12
layout: default
tags: [machine learning]
header:
  image: "/images/nlp.jpg"
excerpt: "Comparison of generative and discriminative models."
toc: true
toc_label: "Content"
---

### Test Title

* For a generative-discriminative pair (NB and LR)
  * The discriminative model has a lower asymptote error
  * The generative model need less training data, more robust to the noise
* Discriminative
  * Model \\( P(y|x) \\)
  * Logistic Regression: $P(y=1|x) = \frac{1}{1+e^{-z}}$
  * Directly learn the (hard or soft) boundary 
  * E.g. Conditional Random Fields (CRF)
* Generative
  * Model $ P(x, y) = P(x|y)P(y) $
  * Usually harder to model \( P(x|y) \Rightarrow \) Naive Bayes assumes conditional independence of \( x_i \)
  * Learn the distribution of the data, or how the data is generated from each class, then determine the most likely class
  * E.g. Naive Bayes, Bayesian Network, Markov Random Field, Hidden Markov Models (HMM), Gaussian Mixture Model
  
$ x_{1,2} = \frac{-5 \pm \sqrt{5^2-12}}{6} $

\( x_{1,2} = \frac{-5 \pm \sqrt{5^2-12}}{6} \)

\\( x_{1,2} = \frac{-5 \pm \sqrt{5^2-12}}{6} \\)

$$ x_{5,3} = \frac{-5 \pm \sqrt{5^2-12}}{6} $$
