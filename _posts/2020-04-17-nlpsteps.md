---
title: "Paper Notes"
date: 2020-05-20
layout: default
tags: [machine learning]
header:
  image: "/images/nlp.jpg"
excerpt: "Short notes for papers I recently read."
toc: true
toc_label: "Content"
---

[Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning](https://arxiv.org/pdf/1910.05396.pdf)
* 2020 ICLR; KAIST & Google 
* Help RL agents generalize on unseen environment with different colors, shapes, or textures
* Input is feed into a single random CONV layer (identity kernel + Xavier initialization)
* Add L2 loss on between clean and randomized features

&nbsp;

[What is the State of Neural Network Pruning?](https://arxiv.org/pdf/2003.03033.pdf)
* 2020; MIT
* Existing work didn't have evaluation from the same standpoints
* In general, Global methods is better than Layerwise methods; Magnitude-based methods are better than Gradient-based methods.
* Proposed a benchmark: [ShrinkBench](https://github.com/jjgo/shrinkbench)

&nbsp;

[HRank: Filter Pruning using High-Rank Feature Map](https://arxiv.org/pdf/2002.10179.pdf)
* 2020 CVPR
* The average rank of multiple feature maps generated by a single filter is always the same
* Estimate ranks by 500 input images
* Prune by filter importance (rank)

&nbsp;

[Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370)
* 2019; Google 
* A pretrained model good for downstream tasks
* Group Normalization (GN): across a few channels of a feature map
* GN + weight standardization > BN when:
  * Training large models with small per-device batches
  * BN not good for transfer (need running mean/var)
 
